{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7795531,"sourceType":"datasetVersion","datasetId":4563812}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jatintec/data-science-championship?scriptVersionId=171865013\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\n\nimport os\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nimport itertools\nimport random\n\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfor dirname, _, filenames in os.walk('/kaggle/input/patent-classifier'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T18:22:12.796327Z","iopub.execute_input":"2024-04-13T18:22:12.796585Z","iopub.status.idle":"2024-04-13T18:22:21.563765Z","shell.execute_reply.started":"2024-04-13T18:22:12.796561Z","shell.execute_reply":"2024-04-13T18:22:21.5629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"train_path = \"/kaggle/input/data-science-student-championship/train.csv\"\ntest_path = \"/kaggle/input/data-science-student-championship/test.csv\"\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:22:21.565374Z","iopub.execute_input":"2024-04-13T18:22:21.565907Z","iopub.status.idle":"2024-04-13T18:22:24.535099Z","shell.execute_reply.started":"2024-04-13T18:22:21.565875Z","shell.execute_reply":"2024-04-13T18:22:24.534236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df['Label'].plot(kind='hist', bins=20, title='Label')\n# plt.gca().spines[['top', 'right',]].set_visible(False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T07:29:24.872503Z","iopub.execute_input":"2024-03-30T07:29:24.872831Z","iopub.status.idle":"2024-03-30T07:29:25.24914Z","shell.execute_reply.started":"2024-03-30T07:29:24.872804Z","shell.execute_reply":"2024-03-30T07:29:25.248279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.groupby('Label').size()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T22:32:14.53418Z","iopub.execute_input":"2024-03-29T22:32:14.534469Z","iopub.status.idle":"2024-03-29T22:32:14.545946Z","shell.execute_reply.started":"2024-03-29T22:32:14.534434Z","shell.execute_reply":"2024-03-29T22:32:14.545131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_=0\n# sum_=0\n# for content in train_df[\"Content\"]:\n#     number_of_words = len(list(content.split(\" \")))\n#     if number_of_words>max_:\n#         max_ = number_of_words\n#     sum_ += number_of_words\n# print(f\"Max tokens: {max_}, Average tokens: {sum_/len(train_df['Content'])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_counts = [len(text.split()) for text in train_df[\"Content\"]]\n\n# # Calculate mean and standard deviation\n# mean_word_count = np.mean(word_counts)\n# std_word_count = np.std(word_counts)\n\n# # Plotting using sns.histplot\n# plt.figure(figsize=(10, 6))\n# sns.histplot(word_counts, kde=True, color='blue', stat='density')\n# plt.title('Distribution of Word Counts in Train Corpus')\n# plt.xlabel('Word Count')\n# plt.ylabel('Density')\n\n# # Optionally, add vertical lines for mean and standard deviations\n# plt.axvline(mean_word_count, color='red', linestyle='--', label=f'Mean: {mean_word_count:.2f}')\n# plt.axvline(mean_word_count + std_word_count, color='green', linestyle='--', label=f'Mean + 1 STD: {mean_word_count + std_word_count:.2f}')\n# plt.axvline(mean_word_count - std_word_count, color='green', linestyle='--', label=f'Mean - 1 STD: {mean_word_count - std_word_count:.2f}')\n\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_=0\n# sum_=0\n# for content in test_df[\"Content\"]:\n#     number_of_words = len(list(content.split(\" \")))\n#     if number_of_words>max_:\n#         max_ = number_of_words\n#     sum_ += number_of_words\n# print(f\"Max tokens: {max_}, Average tokens: {sum_/len(test_df['Content'])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_counts = [len(text.split()) for text in test_df[\"Content\"]]\n\n# # Calculate mean and standard deviation\n# mean_word_count = np.mean(word_counts)\n# std_word_count = np.std(word_counts)\n\n# # Plotting using sns.histplot\n# plt.figure(figsize=(10, 6))\n# sns.histplot(word_counts, kde=True, color='blue', stat='density')\n# plt.title('Distribution of Word Counts in Test Corpus')\n# plt.xlabel('Word Count')\n# plt.ylabel('Density')\n\n# # Optionally, add vertical lines for mean and standard deviations\n# plt.axvline(mean_word_count, color='red', linestyle='--', label=f'Mean: {mean_word_count:.2f}')\n# plt.axvline(mean_word_count + std_word_count, color='green', linestyle='--', label=f'Mean + 1 STD: {mean_word_count + std_word_count:.2f}')\n# plt.axvline(mean_word_count - std_word_count, color='green', linestyle='--', label=f'Mean - 1 STD: {mean_word_count - std_word_count:.2f}')\n\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for label in (1, 2, 3, 4, 5, 6, 7, 8, 9):\n#     train_word_freq = {}\n#     for content in train_df[train_df['Label']==label]['Content']:\n#         for word in content.split():\n#             train_word_freq[word] = train_word_freq.get(word, 0) + 1\n#     # Sort the dictionary by frequency\n#     sorted_word_freq = sorted(train_word_freq.items(), key=lambda x: x[1], reverse=True)\n#     print(f\"Top 10 words with there frequinces: {sorted_word_freq[:10]}, Unique word count {len(sorted_word_freq)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_word_freq = {}\n# for content in test_df['Content']:\n#     for word in content.split():\n#         test_word_freq[word] = test_word_freq.get(word, 0) + 1\n\n# # Sort the dictionary by frequency\n# sorted_word_freq = sorted(test_word_freq.items(), key=lambda x: x[1], reverse=True)\n# print(f\"Top 10 words with there frequinces: {sorted_word_freq[:10]}, Unique word count {len(sorted_word_freq)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Identify common words and compute their frequencies\n# common_words_freq = {word: (train_word_freq[word], test_word_freq[word]) \n#                      for word in train_word_freq if word in test_word_freq}\n\n# # Sort common words by their cumulative frequency (train + test)\n# sorted_common_words_freq = sorted(common_words_freq.items(), \n#                                   key=lambda x: x[1][0] + x[1][1], \n#                                   reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Display the top 10 common words with their frequencies\n# print(f\"Top 10 common words with their frequencies: {sorted_common_words_freq[:10]}\")\n# print(f\"Total unique common words: {len(sorted_common_words_freq)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stop_words = set(stopwords.words('english'))\n# lemmatizer = WordNetLemmatizer()\n# def preprocess_text(text):\n#         # Lemmatize text and split into words\n#         lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in text.split()]\n#         # Remove stopwords and filter out less frequent words\n#         filtered_words = [word for word in lemmatized_words if word not in stop_words and word in vocab]\n#         # Rejoin words into a processed string\n#         return ' '.join(filtered_words)","metadata":{"execution":{"iopub.status.busy":"2024-03-29T23:14:40.921142Z","iopub.execute_input":"2024-03-29T23:14:40.921534Z","iopub.status.idle":"2024-03-29T23:14:40.928704Z","shell.execute_reply.started":"2024-03-29T23:14:40.921503Z","shell.execute_reply":"2024-03-29T23:14:40.927652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_counts = [len(preprocess_text(text).split()) for text in train_df[\"Content\"]]\n# # Calculate mean and standard deviation\n# mean_word_count = np.mean(word_counts)\n# std_word_count = np.std(word_counts)\n\n# # Plotting using sns.histplot\n# plt.figure(figsize=(10, 6))\n# sns.histplot(word_counts, kde=True, color='blue', stat='density')\n# plt.title('Distribution of Word Counts in Train Corpus')\n# plt.xlabel('Word Count')\n# plt.ylabel('Density')\n\n# # Optionally, add vertical lines for mean and standard deviations\n# plt.axvline(mean_word_count, color='red', linestyle='--', label=f'Mean: {mean_word_count:.2f}')\n# plt.axvline(mean_word_count + std_word_count, color='green', linestyle='--', label=f'Mean + 1 STD: {mean_word_count + std_word_count:.2f}')\n# plt.axvline(mean_word_count - std_word_count, color='green', linestyle='--', label=f'Mean - 1 STD: {mean_word_count - std_word_count:.2f}')\n\n# plt.legend()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T23:14:41.542137Z","iopub.execute_input":"2024-03-29T23:14:41.542616Z","iopub.status.idle":"2024-03-29T23:16:07.859903Z","shell.execute_reply.started":"2024-03-29T23:14:41.542573Z","shell.execute_reply":"2024-03-29T23:16:07.858879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_counts = [len(preprocess_text(text).split()) for text in test_df[\"Content\"]]\n# # Calculate mean and standard deviation\n# mean_word_count = np.mean(word_counts)\n# std_word_count = np.std(word_counts)\n\n# # Plotting using sns.histplot\n# plt.figure(figsize=(10, 6))\n# sns.histplot(word_counts, kde=True, color='blue', stat='density')\n# plt.title('Distribution of Word Counts in Test Corpus')\n# plt.xlabel('Word Count')\n# plt.ylabel('Density')\n\n# # Optionally, add vertical lines for mean and standard deviations\n# plt.axvline(mean_word_count, color='red', linestyle='--', label=f'Mean: {mean_word_count:.2f}')\n# plt.axvline(mean_word_count + std_word_count, color='green', linestyle='--', label=f'Mean + 1 STD: {mean_word_count + std_word_count:.2f}')\n# plt.axvline(mean_word_count - std_word_count, color='green', linestyle='--', label=f'Mean - 1 STD: {mean_word_count - std_word_count:.2f}')\n\n# plt.legend()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-29T23:16:07.861842Z","iopub.execute_input":"2024-03-29T23:16:07.862541Z","iopub.status.idle":"2024-03-29T23:16:44.950552Z","shell.execute_reply.started":"2024-03-29T23:16:07.862504Z","shell.execute_reply":"2024-03-29T23:16:44.949687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"# lemmatizer = WordNetLemmatizer()\n# vocab_df = pd.concat([train_df, test_df], ignore_index=True)\n# def build_vocab(texts, min_word_freq=1):\n#     # Flatten all texts into a single list of words and lemmatize them\n#     all_words = [lemmatizer.lemmatize(word.lower()) for text in texts for word in text.split()]\n#     # Count word frequencies\n#     word_freq = Counter(all_words)\n#     # Filter words by minimum frequency\n#     vocab = {word for word, freq in word_freq.items() if freq >= min_word_freq}\n#     return vocab\n# vocab = build_vocab(vocab_df['Content'].to_list())","metadata":{"execution":{"iopub.status.busy":"2024-03-30T07:29:39.191405Z","iopub.execute_input":"2024-03-30T07:29:39.192148Z","iopub.status.idle":"2024-03-30T07:31:44.730714Z","shell.execute_reply.started":"2024-03-30T07:29:39.192115Z","shell.execute_reply":"2024-03-30T07:31:44.729853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_length=512, is_train=True):\n        self.texts = texts\n        self.labels = labels\n        self.is_train = is_train\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        if self.is_train:\n            self.labels = [label - 1 for label in self.labels]\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        item = {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n        }\n        if self.is_train:\n            label = self.labels[idx]\n            item['labels'] = torch.tensor(label, dtype=torch.long)\n        return item","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:22:33.471003Z","iopub.execute_input":"2024-04-13T18:22:33.471366Z","iopub.status.idle":"2024-04-13T18:22:33.480077Z","shell.execute_reply.started":"2024-04-13T18:22:33.471327Z","shell.execute_reply":"2024-04-13T18:22:33.479147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n\n# Convert labels to integers if they're not already (replace 'Label' with your actual label column name)\ntrain_df['Label'] = train_df['Label'].astype(int)\n\ntrain_dataset = TextDataset(\n    texts=train_df[\"Content\"].to_list(),\n    labels=train_df[\"Label\"].to_list(),\n    tokenizer=tokenizer,\n    max_length=256,\n    is_train=True\n)\n# Create DataLoader\n# train_loader = DataLoader(train_dataset, batch_size=16)\n\n# # Iterate through Binary DataLoader\n# for i, batch in enumerate(train_loader):\n#     input_ids = batch['input_ids']\n#     attention_mask = batch['attention_mask']\n#     labels = batch['labels']\n    \n#     # Print out shapes and some example data\n#     print(f\"Batch {i+1}\")\n#     print(f\"Input IDs shape: {input_ids.shape}\")\n#     print(f\"Attention Mask shape: {attention_mask.shape}\")\n#     print(f\"Labels shape: {labels.shape}\")\n#     print(f\"First input IDs: {input_ids[0][:12]}\")\n#     print(f\"First Attention Mask: {attention_mask[0][:12]}\")\n#     print(f\"Number of words\", sum(attention_mask[0]))\n#     print(f\"First Label: {labels[i]}\\n\")\n#     if i == 3:\n#         break\n# print(f\"Length of dataset: \", len(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:22:34.145802Z","iopub.execute_input":"2024-04-13T18:22:34.146179Z","iopub.status.idle":"2024-04-13T18:22:39.254051Z","shell.execute_reply.started":"2024-04-13T18:22:34.146146Z","shell.execute_reply":"2024-04-13T18:22:39.253322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataset\ntest_dataset = TextDataset(\n    texts=test_df[\"Content\"].to_list(),\n    tokenizer=tokenizer,\n    max_length=256,\n    is_train=False\n)\n# test_loader = DataLoader(test_dataset, batch_size=16)\n# Iterate through Binary DataLoader\n# for i, batch in enumerate(test_loader):\n#     input_ids = batch['input_ids']\n#     attention_mask = batch['attention_mask']\n    \n#     print(f\"Batch {i+1}\")\n#     print(f\"Binary Input IDs shape: {input_ids.shape}\")\n#     print(f\"Binary Attention Mask shape: {attention_mask.shape}\")\n#     print(f\"Binary First input IDs: {input_ids[0][:12]}\")\n#     print(f\"Binary First Attention Mask: {attention_mask[0][:12]}\")\n#     print(f\"Number of words\", sum(attention_mask[0]))\n    \n#     if i == 3:\n#         break\n# print(f\"Length of dataset: \", len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:22:39.25531Z","iopub.execute_input":"2024-04-13T18:22:39.255592Z","iopub.status.idle":"2024-04-13T18:22:39.261608Z","shell.execute_reply.started":"2024-04-13T18:22:39.255569Z","shell.execute_reply":"2024-04-13T18:22:39.260627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model","metadata":{}},{"cell_type":"code","source":"scibert = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\nscibert.config.hidden_size","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:32:26.179531Z","iopub.execute_input":"2024-04-13T17:32:26.180103Z","iopub.status.idle":"2024-04-13T17:32:28.768625Z","shell.execute_reply.started":"2024-04-13T17:32:26.180078Z","shell.execute_reply":"2024-04-13T17:32:28.767684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SciBertEmbedder(nn.Module):\n    def __init__(self):\n        super(SciBertEmbedder, self).__init__()\n        self.scibert = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n\n    def forward(self, input_ids, attention_mask=None):\n        with torch.no_grad():\n            outputs = self.scibert(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs[1]  # Output the [CLS] token embeddings\n\nclass Classifier(nn.Module):\n    def __init__(self, hidden_dim, output_dim, dropout_rate):\n        super(Classifier, self).__init__()\n        self.scibert = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc1 = nn.Linear(self.scibert.config.hidden_size, hidden_dim * 2)\n        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)      # Maintaining a funnel-like structure\n        self.fc3 = nn.Linear(hidden_dim, output_dim)         # Output layer\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.scibert(input_ids=input_ids, attention_mask=attention_mask)\n        x = self.fc1(outputs[1])\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = F.gelu(x)        \n        x = self.fc3(x)\n        return x\n\nclass SciBertClassifier(nn.Module):\n    def __init__(self, hidden_dim, output_dim, dropout_rate, device):\n        super(SciBertClassifier, self).__init__()\n        self.classifier = Classifier(hidden_dim, output_dim, dropout_rate).to(device)\n        self.device = device\n\n    def forward(self, input_ids, attention_mask):\n        output = self.classifier(input_ids.to(self.device), attention_mask.to(self.device))\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:25:51.225237Z","iopub.execute_input":"2024-04-13T18:25:51.226079Z","iopub.status.idle":"2024-04-13T18:25:51.236949Z","shell.execute_reply.started":"2024-04-13T18:25:51.226048Z","shell.execute_reply":"2024-04-13T18:25:51.236114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scibert = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n# scibert.to(device)\n# progress_bar = tqdm(train_loader, desc=f\"Test out\")\n# for batch in progress_bar:\n#     input_ids = batch['input_ids'].to(device)\n#     attention_mask = batch['attention_mask'].to(device)\n#     labels = batch['labels'].to(device)\n#     out = scibert(input_ids, attention_mask)\n#     print(out)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:57:19.259608Z","iopub.execute_input":"2024-04-12T15:57:19.260204Z","iopub.status.idle":"2024-04-12T15:57:23.092Z","shell.execute_reply.started":"2024-04-12T15:57:19.260173Z","shell.execute_reply":"2024-04-12T15:57:23.090365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:25:57.145971Z","iopub.execute_input":"2024-04-13T18:25:57.146848Z","iopub.status.idle":"2024-04-13T18:25:57.151756Z","shell.execute_reply.started":"2024-04-13T18:25:57.146812Z","shell.execute_reply":"2024-04-13T18:25:57.150701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train_df.groupby('Label').size()\ntotal_samples = class_counts.sum()\nweights = total_samples / (len(class_counts) * class_counts)\nweights_tensor = torch.tensor(weights.values, dtype=torch.float).to(device)  # Convert Series to numpy array before creating the tensor\nloss_fn = torch.nn.CrossEntropyLoss(weights_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:25:58.32517Z","iopub.execute_input":"2024-04-13T18:25:58.326066Z","iopub.status.idle":"2024-04-13T18:25:58.333935Z","shell.execute_reply.started":"2024-04-13T18:25:58.32603Z","shell.execute_reply":"2024-04-13T18:25:58.333048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = range(1, 10)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:25:59.145026Z","iopub.execute_input":"2024-04-13T18:25:59.145796Z","iopub.status.idle":"2024-04-13T18:25:59.154655Z","shell.execute_reply.started":"2024-04-13T18:25:59.145765Z","shell.execute_reply":"2024-04-13T18:25:59.153736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\ndef calculate_balanced_accuracy(model, data_loader, device):\n    model.eval()  # Set the model to evaluation mode\n    all_preds = []\n    all_true = []\n\n    with torch.no_grad():  # No need to track gradients for inference\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            _, predicted_labels = torch.max(outputs, dim=1)\n\n            # Move predicted and true labels to CPU and then convert to numpy for sklearn compatibility\n            all_preds.extend(predicted_labels.cpu().numpy())\n            all_true.extend(labels.cpu().numpy())\n\n    # Calculate the balanced accuracy score\n    balanced_accuracy = balanced_accuracy_score(all_true, all_preds)\n    return balanced_accuracy, np.array(all_preds), np.array(all_true)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:25:59.365053Z","iopub.execute_input":"2024-04-13T18:25:59.365315Z","iopub.status.idle":"2024-04-13T18:25:59.372613Z","shell.execute_reply.started":"2024-04-13T18:25:59.365294Z","shell.execute_reply":"2024-04-13T18:25:59.371776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"k = 10\nskf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\nepochs = 12\nbatch_size = 16\nearly_stopping_patience = 3\nhidden_dim = 128\noutput_dim = 9\ndropout_rate = 0.2\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass_names = range(1, 10)\n\ndataset_indices = list(range(len(train_dataset)))\ntargets = [train_dataset[i]['labels'].item() for i in dataset_indices]\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(dataset_indices, targets)):\n    print(f'Fold {fold + 1}/{k}')\n    \n    model = SciBertClassifier(hidden_dim, output_dim, dropout_rate, device)\n    optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    val_sampler = SubsetRandomSampler(val_idx)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        train_loader_tqdm = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n        for batch in train_loader_tqdm:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Training Loss: {avg_train_loss:.4f}\")\n        \n        train_balanced_accuracy, train_preds, train_true = calculate_balanced_accuracy(model, train_loader, device)\n        print(f\"Balanced training accuracy: {train_balanced_accuracy:.4f}\")\n        \n        # Calculate and plot training confusion matrix\n#         train_cm = confusion_matrix(train_true, train_preds)\n#         plt.figure(figsize=(10, 7))\n#         plot_confusion_matrix(train_cm, classes=list(class_names), title=f'Training Confusion Matrix, Epoch {epoch+1}')\n#         plt.show()\n\n        model.eval()\n        total_val_loss = 0\n        val_loader_tqdm = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")\n        with torch.no_grad():\n            for batch in val_loader_tqdm:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask)\n                loss = F.cross_entropy(outputs, labels)\n                total_val_loss += loss.item()\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n        \n        val_balanced_accuracy, val_preds, val_true = calculate_balanced_accuracy(model, val_loader, device)\n        print(f\"Balanced validation accuracy: {val_balanced_accuracy:.4f}\")\n        \n        # Calculate and plot validation confusion matrix\n#         val_cm = confusion_matrix(val_true, val_preds)\n#         plt.figure(figsize=(10, 7))\n#         plot_confusion_matrix(val_cm, classes=list(class_names), title=f'Validation Confusion Matrix, Epoch {epoch+1}')\n#         plt.show()\n        \n        scheduler.step(avg_val_loss)\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            model_dir = f'/kaggle/working/model_checkpoints/fold_{fold + 1}'\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir)\n            model_save_path = os.path.join(model_dir, f'model_epoch_{epoch+1}.pth')\n            torch.save(model.state_dict(), model_save_path)\n        else:\n            patience_counter += 1\n\n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping triggered.\")\n            break","metadata":{"execution":{"iopub.status.busy":"2024-04-13T18:26:00.296296Z","iopub.execute_input":"2024-04-13T18:26:00.296668Z","iopub.status.idle":"2024-04-13T18:28:16.086684Z","shell.execute_reply.started":"2024-04-13T18:26:00.29664Z","shell.execute_reply":"2024-04-13T18:28:16.085412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# state_dict = torch.load(\"/kaggle/input/patent-classifier/pytorch/v3/1/model_epoch_2.pth\")\n# model.load_state_dict(state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}